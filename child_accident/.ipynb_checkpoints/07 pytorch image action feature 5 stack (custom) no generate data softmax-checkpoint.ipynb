{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    def __init__(self, seq_length, batch_size):\n",
    "        np.random.seed(100)\n",
    "        self.seq_length = seq_length\n",
    "        self.acc_num = 334\n",
    "        self.no_acc_num = 392\n",
    "        self.split = 0.95\n",
    "        self.train_index = 0 \n",
    "        self.train_batch_size = batch_size\n",
    "        self.valid_index = 0\n",
    "        self.valid_batch_size = 10\n",
    "        self.read_annotation()\n",
    "        self.shuffle_data()\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        #img_range = np.arange(0,300-self.seq_length-60,60)\n",
    "        acc_list = np.arange(1,self.acc_num+1)\n",
    "        no_acc_list = np.arange(1,self.no_acc_num+1)\n",
    "        list1 = np.array(np.meshgrid(1,acc_list)).T.reshape(-1,2)\n",
    "        list2 = np.array(np.meshgrid(0,no_acc_list)).T.reshape(-1,2)\n",
    "        shuffle_list = np.concatenate([list1, list2], axis=0)\n",
    "        np.random.shuffle(shuffle_list)\n",
    "        self.train = shuffle_list[:int(shuffle_list.shape[0]*self.split)]\n",
    "        self.valid = shuffle_list[int(shuffle_list.shape[0]*self.split):] \n",
    "        \n",
    "    def read_annotation(self):\n",
    "        annotation_file = '/media/user/Hard_Disk/Dataset/child_accident_2/annotation/accident_frame.txt'\n",
    "        w = open(annotation_file, \"r\")\n",
    "        ann = w.read()\n",
    "        annotation_data = []\n",
    "        for i in ann.split(\"\\n\"):\n",
    "            b = i.split(\" \")\n",
    "            if (len(b) > 1):\n",
    "                annotation_data.append(b[1])\n",
    "        self.annotation = np.array(annotation_data).astype(\"int32\")\n",
    "        \n",
    "    def read_data(self, is_accident, dir_index):\n",
    "        data = []\n",
    "        label = []\n",
    "        img_path = '/media/user/Hard_Disk/Dataset/child_accident_2/image feature/' \n",
    "        act_path = '/media/user/Hard_Disk/Dataset/child_accident_2/action feature/' \n",
    "        if (is_accident):\n",
    "            acc_dir = \"accident/\"\n",
    "            range_start = self.annotation[dir_index-1] - self.seq_length\n",
    "            range_end = self.annotation[dir_index-1]\n",
    "            label.append([0,1])\n",
    "            \n",
    "        else:\n",
    "            acc_dir = \"no_accident/\"\n",
    "            range_start = 0\n",
    "            range_end = self.seq_length\n",
    "            label.append([1,0])\n",
    "            \n",
    "        dir_name = \"%04d\"%dir_index\n",
    "        img_npy = []\n",
    "        act_npy = []\n",
    "        for j in range(range_start, range_end):\n",
    "            img_feature = np.load(img_path + acc_dir + dir_name + \"/\" + str(j) + \".npy\")\n",
    "            img_npy.append(img_feature)\n",
    "            act_feature = np.load(act_path + acc_dir + dir_name + \"/\" + str(j) + \".npy\")\n",
    "            act_npy.append(act_feature)\n",
    "        \n",
    "            \n",
    "        return np.array(img_npy), np.array(act_npy) ,np.array(label)\n",
    "        \n",
    "    def next_batch(self, mode=\"train\"):\n",
    "        batch_img = []\n",
    "        batch_act = []\n",
    "        batch_y = []\n",
    "        if (mode == \"valid\"):\n",
    "            batch_size = self.valid_batch_size\n",
    "        elif (mode == \"train\"):\n",
    "            batch_size = self.train_batch_size\n",
    "        for i in range(batch_size):\n",
    "            if (mode == \"train\"):\n",
    "                img, act, label = self.read_data(self.train[self.train_index+i][0], self.train[self.train_index+i][1])\n",
    "            elif (mode == \"valid\"):\n",
    "                img, act, label = self.read_data(self.valid[self.valid_index+i][0], self.valid[self.valid_index+i][1])\n",
    "            batch_img.append(img)\n",
    "            batch_act.append(act)\n",
    "            batch_y.append(label)\n",
    "        if (mode == \"valid\"):\n",
    "            self.valid_index += self.valid_batch_size\n",
    "        elif (mode == \"train\"):\n",
    "            self.train_index += self.train_batch_size\n",
    "        return np.array(batch_img), np.array(batch_act), np.squeeze(np.array(batch_y))\n",
    "    \n",
    "    def has_next(self, mode=\"train\"):\n",
    "        if (mode == \"train\"):\n",
    "            if (self.train_index + self.train_batch_size >= self.train.shape[0]):\n",
    "                return False\n",
    "        elif (mode == \"valid\"):\n",
    "            if (self.valid_index + self.valid_batch_size >= self.valid.shape[0]):\n",
    "                return False\n",
    "        return True\n",
    "    def display_shape(self):\n",
    "        print(\"train shape:\",self.train.shape, \" valid shape:\",self.valid.shape)\n",
    "        \n",
    "    def reset_batch(self, mode=\"train\"):\n",
    "        if (mode == \"train\"):\n",
    "            self.train_index = 0\n",
    "            np.random.shuffle(self.train)\n",
    "        elif (mode == \"valid\"):\n",
    "            self.valid_index = 0\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, img_input_size, act_input_size, hidden_size, embedding_size, embedding_size2, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.img = nn.Sequential(\n",
    "            nn.Linear(img_input_size, embedding_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size2)\n",
    "        )\n",
    "        self.act = nn.Sequential(\n",
    "            nn.Linear(act_input_size, embedding_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size2)\n",
    "        )\n",
    "            \n",
    "        \n",
    "        self.img_fc1 = nn.Linear(img_input_size, embedding_size)\n",
    "        self.act_fc1 = nn.Linear(act_input_size, embedding_size)\n",
    "        self.img_fc2 = nn.Linear(embedding_size, embedding_size2)\n",
    "        self.act_fc2 = nn.Linear(embedding_size, embedding_size2)\n",
    "        self.lstm = nn.LSTM(embedding_size2*2, hidden_size, num_layers,dropout=0.25, batch_first=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # Set initial hidden and cell states \n",
    "        x1 = self.img_fc1(x)\n",
    "        x1 = self.img_fc2(x1)\n",
    "        x2 = self.act_fc1(y)\n",
    "        x2 = self.act_fc2(x2)\n",
    "#         x1 = self.img(x)\n",
    "#         x = self.act(y)\n",
    "        x = torch.cat((x1, x2), 2)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc2(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNN(img_input_size=20*4096, act_input_size=1024, hidden_size=1024, embedding_size=1024, embedding_size2=512, num_layers=1, num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.from_numpy(np.array([1,1])).type(torch.FloatTensor).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (689, 2)  valid shape: (37, 2)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 100\n",
    "num_epochs = 20\n",
    "train_data = data(seq_length=sequence_length, batch_size=50)\n",
    "train_data.display_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    i = 0\n",
    "    acc = 0\n",
    "    while(train_data.has_next(\"valid\")):\n",
    "        img, act, labels = train_data.next_batch(\"valid\")\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        img = torch.from_numpy(img.reshape(-1, sequence_length, 20*4096)).to(device)\n",
    "        act = torch.from_numpy(np.squeeze(act)).to(device)\n",
    "        outputs = model(img, act)\n",
    "        outputs = F.softmax(outputs)\n",
    "        predict = torch.max(outputs, 1)[1]\n",
    "        target = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, target)\n",
    "        correct = (predict == target).squeeze()\n",
    "        acc += torch.nonzero(correct).size(0) / predict.shape[0]\n",
    "        i += 1\n",
    "    train_data.reset_batch(\"valid\")\n",
    "    print(\"validation accuracy:\",acc/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step 5, Loss: 0.6800\n",
      "train accuracy: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.5333333333333333\n",
      "====================================\n",
      "Epoch [1/20], Step 10, Loss: 0.6690\n",
      "train accuracy: 0.5244444444444444\n",
      "validation accuracy: 0.5\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Epoch [2/20], Step 5, Loss: 0.7010\n",
      "train accuracy: 0.6\n",
      "validation accuracy: 0.4666666666666666\n",
      "====================================\n",
      "Epoch [2/20], Step 10, Loss: 0.6626\n",
      "train accuracy: 0.6199999999999999\n",
      "validation accuracy: 0.5\n",
      "====================================\n",
      "epoch: 2\n",
      "Epoch [3/20], Step 5, Loss: 0.6525\n",
      "train accuracy: 0.5950000000000001\n",
      "validation accuracy: 0.4666666666666666\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "# Train the model\n",
    "\n",
    "for epoch in range(100):\n",
    "    i = 0\n",
    "    acc = 0\n",
    "    print(\"epoch:\",epoch)\n",
    "    while (train_data.has_next(\"train\")):\n",
    "        img, act, labels = train_data.next_batch(\"train\")\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        \n",
    "        img = torch.from_numpy(img.reshape(-1, sequence_length, 20*4096)).to(device)\n",
    "        act = torch.from_numpy(np.squeeze(act)).to(device)\n",
    "        outputs = model(img, act)\n",
    "        outputs = F.softmax(outputs)\n",
    "#         outputs, torch.max(labels, 1)[1]\n",
    "        \n",
    "        predict = torch.max(outputs, 1)[1]\n",
    "        target = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        \n",
    "        correct = (predict == target).squeeze()\n",
    "#         print(predict)\n",
    "#         print(target)\n",
    "#         print(\"================\")\n",
    "#         print(correct)\n",
    "        acc += torch.nonzero(correct).size(0) / predict.shape[0]\n",
    "        \n",
    "    \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        if (i+1) % 5 == 0:\n",
    "            \n",
    "            print ('Epoch [{}/{}], Step {}, Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, loss.item()))\n",
    "            print ('train accuracy:',acc/i)\n",
    "            evaluation(model)\n",
    "            print(\"====================================\")\n",
    "    if (epoch % 10 == 0):\n",
    "        path = \"pytorch/model-\"+str(epoch)+\".ckpt\"\n",
    "        torch.save(model, path)\n",
    "    train_data.reset_batch(\"train\")\n",
    "# # Test the model\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
