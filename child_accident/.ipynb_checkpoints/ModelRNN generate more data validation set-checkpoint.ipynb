{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "from tensorflow.contrib import rnn\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_image = \"/media/user/Hard_Disk/Dataset/child_accident_2/clip image/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.acc_num = 334\n",
    "        self.no_acc_num = 392\n",
    "        self.index = 0\n",
    "        self.seq_length = 50\n",
    "        self.batch_size = batch_size\n",
    "        self.read_annotation()\n",
    "        self.shuffle_data()\n",
    "        \n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        img_range = np.arange(0,300-self.seq_length-30,30)\n",
    "        acc_list = np.arange(1,self.acc_num+1)\n",
    "        no_acc_list = np.arange(1,self.no_acc_num+1)\n",
    "        list1 = np.array(np.meshgrid(1,acc_list,img_range)).T.reshape(-1,3)\n",
    "        list2 = np.array(np.meshgrid(0,no_acc_list,img_range)).T.reshape(-1,3)\n",
    "        self.shuffle_list = np.concatenate([list1, list2], axis=0)\n",
    "        np.random.shuffle(self.shuffle_list)\n",
    "        \n",
    "    def read_annotation(self):\n",
    "        annotation_file = '/media/user/Hard_Disk/Dataset/child_accident_2/annotation/accident_frame.txt'\n",
    "        w = open(annotation_file, \"r\")\n",
    "        ann = w.read()\n",
    "        annotation_data = []\n",
    "        for i in ann.split(\"\\n\"):\n",
    "            b = i.split(\" \")\n",
    "            if (len(b) > 1):\n",
    "                annotation_data.append(b[1])\n",
    "        self.annotation = np.array(annotation_data).astype(\"int32\")\n",
    "        \n",
    "    def read_data(self, is_accident, dir_index, image_range):\n",
    "        data = []\n",
    "        label = []\n",
    "        if (is_accident):\n",
    "            acc_dir = \"accident/\"\n",
    "        else:\n",
    "            acc_dir = \"no_accident/\"\n",
    "        dir_name = \"%04d\"%dir_index\n",
    "        for j in range(image_range, image_range+self.seq_length):\n",
    "#             print(self.path+ acc_dir + dir_name +\"/\"+ str(j)+\".jpg\")\n",
    "            img = cv2.imread(self.path+ acc_dir + dir_name +\"/\"+ str(j)+\".jpg\")\n",
    "#             print(img.shape)\n",
    "            imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            imgRGB = cv2.resize(imgRGB, (150, 150))\n",
    "            data.append(imgRGB)\n",
    "        \n",
    "        if (is_accident and ((image_range + self.seq_length + 30) > self.annotation[dir_index-1])):\n",
    "            label.append([0,1])\n",
    "        else:\n",
    "            label.append([1,0])\n",
    "        return np.array(data),np.array(label)\n",
    "    def has_next(self):\n",
    "        if (self.index + self.batch_size <= self.shuffle_list.shape[0]):\n",
    "            return True\n",
    "    def reset(self):\n",
    "        self.shuffle_data\n",
    "        self.index = 0\n",
    "        \n",
    "    def next_batch(self):     \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for i in range(self.batch_size):\n",
    "            data, label = self.read_data(self.shuffle_list[self.index+i][0], self.shuffle_list[self.index+i][1], self.shuffle_list[self.index+i][2])\n",
    "            batch_x.append(data)\n",
    "            batch_y.append(label)\n",
    "        self.index += self.batch_size\n",
    "        return np.array(batch_x), np.squeeze(np.array(batch_y))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRNN():\n",
    "    def __init__(self, img_size_h, img_size_w, hidden_num, rnn_size, max_seq_sz, num_layers, n_classes):\n",
    "        self.img_size_h = img_size_h\n",
    "        self.img_size_w = img_size_w\n",
    "        self.hidden_num = hidden_num\n",
    "        self.rnn_size = rnn_size\n",
    "        self.input_seq = tf.placeholder('float', [None, max_seq_sz, img_size_h, img_size_w, 3])\n",
    "        self.target = tf.placeholder('float', [None, n_classes])\n",
    "        self.max_seq_sz = max_seq_sz\n",
    "        self.num_layers = num_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.__build()\n",
    "        \n",
    "        \n",
    "    def __weight_variable(self, shape, myName):\n",
    "        initial = tf.random_normal(shape, stddev=0.1, name=myName)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    \n",
    "    def __bias_variable(self, shape, myName):\n",
    "        initial = tf.constant(0.1, shape=shape, name=myName)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def __build(self):\n",
    "        w_fc_in = self.__weight_variable([self.img_size_h*self.img_size_w*3, self.hidden_num], 'w_fc_in')\n",
    "        b_fc_in = self.__bias_variable([self.hidden_num], 'b_fc_in')\n",
    "        \n",
    "        w_fc_o = self.__weight_variable([self.rnn_size, self.hidden_num], 'w_fc_o')\n",
    "        b_fc_o = self.__bias_variable([self.hidden_num], 'b_fc_o')\n",
    "                \n",
    "        w_output_action = self.__weight_variable([self.hidden_num, self.n_classes], 'w_fc_in')\n",
    "        b_output_action = self.__bias_variable([self.n_classes], 'b_fc_in')\n",
    "           \n",
    "        x = tf.reshape(self.input_seq, [-1, self.img_size_h*self.img_size_w*3])\n",
    "\n",
    "        h1 = tf.nn.relu(tf.matmul(x, w_fc_in) + b_fc_in)\n",
    "        h1 = tf.reshape(h1, [-1, self.max_seq_sz, self.hidden_num])\n",
    "        \n",
    "        #rnn\n",
    "        h1 = tf.unstack(h1, axis=1)\n",
    "        def get_cell():\n",
    "            return rnn.GRUCell(self.rnn_size)   \n",
    "        gru_cell = rnn.MultiRNNCell([get_cell() for _ in range(self.num_layers)])\n",
    "        outputs, states = rnn.static_rnn(gru_cell, h1, dtype=tf.float32) \n",
    "        #fc_o\n",
    "        h2 = tf.nn.relu(tf.matmul(outputs[-1], w_fc_o) + b_fc_o)\n",
    "        #output\n",
    "#         output_label = tf.nn.softmax(tf.matmul(h2, w_output_action) + b_output_action)\n",
    "        output_label = tf.matmul(h2, w_output_action) + b_output_action\n",
    "        #    \n",
    "        self.correct_prediction = tf.equal(tf.argmax(output_label,1), tf.argmax(self.target, 1))\n",
    "        \n",
    "        \n",
    "        self.prediction = output_label\n",
    "        self.saver = tf.train.Saver(write_version=tf.train.SaverDef.V2, max_to_keep=100)\n",
    "        \n",
    "    def train(self, sess, model_save_path, batch_gen, n_epochs, save_freq):\n",
    "        accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.target, logits=self.prediction))\n",
    "        optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "#         saver.restore(sess, \"RNN_model/epoch-1/model.ckpt\")\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            i=0\n",
    "            tStart_epoch = time.time()\n",
    "            while(batch_gen.has_next()):\n",
    "#                 print(\"batch:\",i+1)\n",
    "                batch_in, batch_target = batch_gen.next_batch()\n",
    "#                 print(batch_in.shape, batch_target.shape)\n",
    "#                 print(batch_in[0])\n",
    "                _, err, acc = sess.run([optimizer, loss, accuracy], feed_dict={self.input_seq: batch_in, self.target: batch_target})\n",
    "#                 a = sess.run([self.pred], feed_dict={self.input_seq: batch_in, self.target: batch_target} )\n",
    "                i=i+1\n",
    "                epoch_loss += err   \n",
    "                epoch_acc += acc\n",
    "            tStop_epoch = time.time()\n",
    "            batch_gen.reset()\n",
    "            print (\"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\")\n",
    "            print ('epoch loss:',(epoch_loss/i))\n",
    "            print ('epoch acc:',(epoch_acc/i))\n",
    "            if epoch%save_freq==0:  \n",
    "                print ('Epoch', (epoch+1), 'completed out of ',n_epochs,'epoch loss:',(epoch_loss/i),' epoch acc:',(epoch_acc/i))\n",
    "                \n",
    "                path = model_save_path+\"/epoch-\"+str(epoch+1)\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                saver.save(sess, path+\"/model.ckpt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRNN(img_size_h=150, img_size_w=150, hidden_num=1024, rnn_size=512, max_seq_sz= 50, num_layers=1, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = data(clip_image, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c6ad21c69de0>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch Time Cost: 1031.38 s\n",
      "epoch loss: 0.7211984892310532\n",
      "epoch acc: 0.8190426952705896\n",
      "Epoch 1 completed out of  20 epoch loss: 0.7211984892310532  epoch acc: 0.8190426952705896\n",
      "Epoch Time Cost: 1022.91 s\n",
      "epoch loss: 0.5052014115607114\n",
      "epoch acc: 0.8491735487622171\n",
      "Epoch 2 completed out of  20 epoch loss: 0.5052014115607114  epoch acc: 0.8491735487622171\n",
      "Epoch Time Cost: 1024.05 s\n",
      "epoch loss: 0.42714721476707285\n",
      "epoch acc: 0.871212115973111\n",
      "Epoch 3 completed out of  20 epoch loss: 0.42714721476707285  epoch acc: 0.871212115973111\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model.train(sess, model_save_path=\"RNN_model\", batch_gen=batch_gen, n_epochs=20, save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
