{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz = '/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/training/'\n",
    "test_npz = '/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/testing/'\n",
    "train_path = \"/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/training/\"\n",
    "test_path = \"/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/testing/\"\n",
    "save_path = \"./model2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "# parameter\n",
    "def build_model():\n",
    "    batch_size = 6\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # feature dimension\n",
    "    n_frames = 100\n",
    "    n_frames_2 = 100\n",
    "    n_detection = 20\n",
    "    n_image_feature = 4096\n",
    "    n_action_feature = 1024\n",
    "\n",
    "    # hidden layer\n",
    "    n_image_hidden = 256\n",
    "    n_action_hidden = 128\n",
    "    n_att_hidden = 256\n",
    "    # n_score_hidden = 32\n",
    "    n_hidden = 640\n",
    "    n_classes = 2\n",
    "    #####################\n",
    "    #       input       #\n",
    "    #####################\n",
    "    x_image = tf.placeholder(\"float\",[None, n_frames, n_detection, n_image_feature])\n",
    "    x_action = tf.placeholder(\"float\",[None, n_frames, 1, n_action_feature])\n",
    "    # x_score = tf.placeholder(\"float\", [None, n_frames, 1])\n",
    "    keep = tf.placeholder(\"float\",[None])\n",
    "    # y_score = tf.placeholder(\"float\",[None, n_frames, n_classes])\n",
    "#     y_label = tf.placeholder(\"float\",[None, 1])\n",
    "    y_class = tf.placeholder(\"float\",[None, n_classes])\n",
    "    #####################\n",
    "    #       weight      #\n",
    "    #####################\n",
    "    weights = {\n",
    "        'image': tf.Variable(tf.random_normal([n_image_feature,n_image_hidden], mean=0.0, stddev=0.01)),\n",
    "        'object': tf.Variable(tf.random_normal([n_image_feature,n_att_hidden], mean=0.0, stddev=0.01)),\n",
    "        'action': tf.Variable(tf.random_normal([n_action_feature,n_action_hidden], mean=0.0, stddev=0.01)),\n",
    "    #     'score': tf.Variable(tf.random_normal([1,n_score_hidden], mean=0.0, stddev=0.01)),\n",
    "        'att_w': tf.Variable(tf.random_normal([n_att_hidden, 1], mean=0.0, stddev=0.01)),\n",
    "        'att_wa': tf.Variable(tf.random_normal([n_hidden, n_att_hidden], mean=0.0, stddev=0.01)),\n",
    "        'att_ua': tf.Variable(tf.random_normal([n_att_hidden, n_att_hidden], mean=0.0, stddev=0.01)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=0.0, stddev=0.01)),\n",
    "        'decoder_out': tf.Variable(tf.random_normal([n_hidden, 1], mean=0.0, stddev=0.01)),\n",
    "        'stack_1to3': tf.Variable(tf.random_normal([n_hidden*2, n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'stack_1to2': tf.Variable(tf.random_normal([n_hidden*2, n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'stack_2to2': tf.Variable(tf.random_normal([n_hidden*2, n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'stack_2to3': tf.Variable(tf.random_normal([n_hidden*2, n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'stack_3to3': tf.Variable(tf.random_normal([n_hidden*2, n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'predict_class': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=0.0, stddev=0.01))\n",
    "    }\n",
    "    biases = {\n",
    "        'image': tf.Variable(tf.random_normal([n_image_hidden], mean=0.0, stddev=0.01)),\n",
    "        'object': tf.Variable(tf.random_normal([n_att_hidden], mean=0.0, stddev=0.01)),\n",
    "        'action': tf.Variable(tf.random_normal([n_action_hidden], mean=0.0, stddev=0.01)),\n",
    "    #     'score': tf.Variable(tf.random_normal([n_score_hidden], mean=0.0, stddev=0.01)),\n",
    "        'att_ba': tf.Variable(tf.zeros([n_att_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([1], mean=0.0, stddev=0.01)),\n",
    "        'decoder_out': tf.Variable(tf.random_normal([1], mean=0.0, stddev=0.01)),\n",
    "        'fusion_2': tf.Variable(tf.random_normal([n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'fusion_3': tf.Variable(tf.random_normal([n_hidden*2], mean=0.0, stddev=0.01)),\n",
    "        'predict_class': tf.Variable(tf.random_normal([n_classes], mean=0.0, stddev=0.01))\n",
    "\n",
    "    }\n",
    "\n",
    "    #####################\n",
    "    #        LSTM       #\n",
    "    #####################\n",
    "    #      Encoder      #\n",
    "    #####################\n",
    "    lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden,initializer= tf.random_normal_initializer(mean=0.0,stddev=0.01),use_peepholes = True,state_is_tuple = False)\n",
    "    lstm_cell_stack_2 = tf.nn.rnn_cell.LSTMCell(n_hidden,initializer= tf.random_normal_initializer(mean=0.0,stddev=0.01),use_peepholes = True,state_is_tuple = False)\n",
    "    lstm_cell_stack_3 = tf.nn.rnn_cell.LSTMCell(n_hidden,initializer= tf.random_normal_initializer(mean=0.0,stddev=0.01),use_peepholes = True,state_is_tuple = False)\n",
    "\n",
    "\n",
    "    lstm_cell_dropout = tf.nn.rnn_cell.DropoutWrapper(lstm_cell,output_keep_prob=1 - keep[0])\n",
    "    lstm_cell_stack_2_dropout = tf.nn.rnn_cell.DropoutWrapper(lstm_cell_stack_2,output_keep_prob=1 - keep[0])\n",
    "    lstm_cell_stack_3_dropout = tf.nn.rnn_cell.DropoutWrapper(lstm_cell_stack_3,output_keep_prob=1 - keep[0])\n",
    "\n",
    "\n",
    "\n",
    "    istate = tf.zeros([batch_size, lstm_cell.state_size])\n",
    "    h_prev = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "    stack_istate_2 = tf.zeros([batch_size, lstm_cell_stack_2.state_size])\n",
    "    stack_h_prev_2 = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "    stack_istate_3 = tf.zeros([batch_size, lstm_cell_stack_3.state_size])\n",
    "    stack_h_prev_3 = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "    zeros_object = tf.to_float(tf.not_equal(tf.reduce_sum(tf.transpose(x_image[:,:,1:n_detection,:],[1,2,0,3]),3),0)) # frame x n x b\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(n_frames):\n",
    "        # x_image: (n_batch, n_frame, n_detection, n_feature): (10, 300, 20, 4096)\n",
    "        X_image = tf.transpose(x_image[:,i,:,:], [1, 0, 2])   # shape:(20, 10, 4096)\n",
    "        X_action = tf.transpose(x_action[:,i,:,:], [1, 0, 2]) # shape:(1, 10, 4096)\n",
    "\n",
    "        image = tf.matmul(X_image[0,:,:],weights['image']) + biases['image']\n",
    "        action = tf.matmul(X_action[0,:,:],weights['action']) + biases['action']\n",
    "    #     score = tf.matmul(x_score[:,i], weights['score']) + biases['score']\n",
    "\n",
    "        # object embedding\n",
    "        n_object = tf.reshape(X_image[1:n_detection,:,:], [-1, n_image_feature])  # (19, 10, 4096) -> (190, 4096)\n",
    "        n_object = tf.matmul(n_object, weights['object']) + biases['object'] # (190, 4096) * (4096, n_att_hidden) + (n_hidden) = (190, n_att_hidden)\n",
    "        n_object = tf.reshape(n_object,[n_detection-1,batch_size,n_att_hidden]) # Reshape -> (19, 10, n_att_hidden)\n",
    "        n_object = tf.multiply(n_object,tf.expand_dims(zeros_object[i],2))\n",
    "\n",
    "        # object attention\n",
    "        brcst_w = tf.tile(tf.expand_dims(weights['att_w'], 0), [n_detection-1,1,1])\n",
    "        image_part = tf.matmul(n_object, tf.tile(tf.expand_dims(weights['att_ua'], 0), [n_detection-1,1,1])) + biases['att_ba'] # n x b x h\n",
    "        e = tf.tanh(tf.matmul(h_prev,weights['att_wa'])+image_part) # n x b x h    \n",
    "        alphas = tf.multiply(tf.nn.softmax(tf.reduce_sum(tf.matmul(e,brcst_w),2),0),zeros_object[i])\n",
    "        # weighting sum\n",
    "        attention_list = tf.multiply(tf.expand_dims(alphas,2),n_object)\n",
    "        attention = tf.reduce_sum(attention_list,0) # b x h\n",
    "\n",
    "        # concat frame & object\n",
    "        fusion = tf.concat([image , attention, action], 1)\n",
    "\n",
    "#         if i > 0 :  tf.get_variable_scope().reuse_variables()\n",
    "        states_all = []\n",
    "        outputs_all = []\n",
    "        with tf.variable_scope(\"LSTM\") as vs:\n",
    "            outputs,istate = lstm_cell_dropout(fusion,istate)\n",
    "            lstm_variables = [v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "\n",
    "        with tf.variable_scope(\"LSTM_stack_2\") as vs:\n",
    "            if (i % 15 == 0):\n",
    "                stack_outputs_2, stack_istate_2 = lstm_cell_stack_2_dropout(fusion, stack_istate_2)\n",
    "                origin_fusion_1 = fusion\n",
    "            else:\n",
    "                stack_outputs_2, stack_istate_2 = lstm_cell_stack_2_dropout(origin_fusion_1, stack_istate_2)\n",
    "            lstm_variables_2 = [v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "\n",
    "        with tf.variable_scope(\"LSTM_stack_3\") as vs:\n",
    "            if (i % 30 == 0):\n",
    "                stack_outputs_3, stack_istate_3 = lstm_cell_stack_3_dropout(fusion, stack_istate_3)\n",
    "                origin_fusion_2 = fusion\n",
    "            else:\n",
    "                stack_outputs_3, stack_istate_3 = lstm_cell_stack_3_dropout(origin_fusion_2, stack_istate_3)\n",
    "            lstm_variables_3 = [v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "\n",
    "        stack_istate_3 = tf.matmul(istate,weights['stack_1to3']) + tf.matmul(stack_istate_2,weights['stack_2to3']) + tf.matmul(stack_istate_3,weights['stack_3to3']) + biases['fusion_3']\n",
    "        stack_istate_2 = tf.matmul(istate,weights['stack_1to2']) + tf.matmul(stack_istate_2,weights['stack_2to2']) + biases['fusion_2']\n",
    "    predict_class = tf.nn.softmax(tf.matmul(stack_outputs_3, weights['predict_class']) + biases['predict_class'])\n",
    "    class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_class, logits=predict_class))\n",
    "    loss = class_loss\n",
    "    correct_prediction = tf.equal(tf.argmax(predict_class,1), tf.argmax(y_class, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     predict_frame = tf.matmul(stack_outputs_3,weights['out']) + biases['out']\n",
    "#     prediction = tf.transpose(prediction)\n",
    "    # loss = tf.cond(tf.less(prediction, y_label), tf.exp(prediction - y_label), tf.exp(2*(prediction - y_label)))\n",
    "#     frame_loss = tf.reduce_mean(tf.square(predict_frame - y_label))\n",
    "#     loss = class_loss + 0.1 * frame_loss\n",
    "    \n",
    "    \n",
    "\n",
    "    #     # save prev hidden state of LSTM\n",
    "    #     h_prev = outputs\n",
    "\n",
    "    #     # FC to output\n",
    "    # #     pred = tf.matmul(outputs,weights['out']) + biases['out'] # b x n_classes\n",
    "    #     prediction = tf.nn.softmax(tf.matmul(outputs,weights['out']) + biases['out'])\n",
    "    #     temp_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_score[:,i], logits=prediction))\n",
    "    #     loss = tf.add(loss, temp_loss)\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss/n_frames_2)\n",
    "    #     states_all.append(istate)\n",
    "    #     outputs_all.append(outputs)\n",
    "    return x_image, x_action, y_class, keep, loss, optimizer, predict_class, accuracy\n",
    "    # #####################\n",
    "    # #      Decoder      #\n",
    "    # #####################\n",
    "    # lstm_cell_2 = tf.nn.rnn_cell.LSTMCell(n_hidden,initializer= tf.random_normal_initializer(mean=0.0,stddev=0.01),use_peepholes = True,state_is_tuple = False)\n",
    "    # lstm_cell_dropout_2 = tf.nn.rnn_cell.DropoutWrapper(lstm_cell_2,output_keep_prob=1 - keep[0])\n",
    "    # istate = states_all[-1]\n",
    "    # h_prev = outputs_all[-1]\n",
    "    # loss = []\n",
    "    # for i in range(n_frames_2):\n",
    "    # #     print(i)\n",
    "    # #     tf.get_variable_scope().reuse_variables()\n",
    "    #     with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as vs:\n",
    "    #         outputs,istate = lstm_cell_dropout_2(h_prev,istate)\n",
    "    #         lstm_variables = [v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "    #     # save prev hidden state of LSTM\n",
    "    #     h_prev = outputs\n",
    "    #     # FC to output\n",
    "    #     pred = tf.matmul(outputs,weights['decoder_out']) + biases['decoder_out'] # b x n_classes\n",
    "\n",
    "    #     temp_loss = tf.reduce_mean(tf.square(pred - y_score[:,i]))\n",
    "    #     loss = tf.add(loss, temp_loss)\n",
    "    # with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    #     optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss/n_frames_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/testing/\"\n",
    "batch_data = np.load(path+'batch_001.npz')\n",
    "# batch_image_x = batch_data['data'][:,:150]\n",
    "# batch_action_x = batch_data['action'][:,:150]\n",
    "# batch_score_y = batch_data['score'][:,150:]\n",
    "# batch_score_y  = revise_score(batch_score_y)\n",
    "# batch_score_y = np.expand_dims(batch_score_y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"123\"\n",
    "b = int(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([246, 223, 271,  16,  81, 140], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data['id'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '/media/user/Hard_Disk/Dataset/child_accident_2/annotation/accident_frame.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open(annotation_file, \"r\")\n",
    "ann = w.read()\n",
    "annotation_data = []\n",
    "for i in ann.split(\"\\n\"):\n",
    "    b = i.split(\" \")\n",
    "    if (len(b) > 1):\n",
    "        annotation_data.append(b[1])\n",
    "annotation_data = np.array(annotation_data).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_score(data):\n",
    "    label = []\n",
    "    scope1 = np.array([1,0])\n",
    "    scope2 = np.array([0,1])\n",
    "    for i in range(data.shape[0]):\n",
    "        isAccident = False\n",
    "        for j in range(data.shape[1]):\n",
    "            if (data[i][j][1] == 1):\n",
    "                if (j>87 and j < 117):\n",
    "                    label.append(scope2)\n",
    "                    isAccident = True\n",
    "                    break\n",
    "        if (isAccident == False):\n",
    "            label.append(scope1)\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data):\n",
    "    batch_image_x = []\n",
    "    batch_action_x = [] \n",
    "    batch_score_y = []\n",
    "    for i in range(6):\n",
    "        if (data['label'][i][0] == 1):\n",
    "            batch_action_x.append(data['action'][i][50:150])\n",
    "            batch_image_x.append(data['data'][i][50:150])\n",
    "            batch_score_y.append([1,0])\n",
    "        else:\n",
    "            start = annotation_data[int(data['id'][i])-1]\n",
    "            batch_action_x.append(data['action'][i][start-100:start])\n",
    "            batch_image_x.append(data['data'][i][start-100:start])\n",
    "            batch_score_y.append([0,1])\n",
    "    return np.array(batch_image_x), np.array(batch_action_x), np.array(batch_score_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(sess, num, path, x_image, x_action, y_class, keep, loss, optimizer, predict_class, accuracy):\n",
    "    total_loss = 0.0\n",
    "    batch_size = 6\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    for batch in range(1,num+1):\n",
    "        file_name = '%03d' %batch\n",
    "        batch_data = np.load(train_path+'batch_'+file_name+'.npz')\n",
    "        batch_image_x, batch_action_x, batch_score_y = build_data(batch_data)\n",
    "        _,batch_loss, predict_accident, acc = sess.run([optimizer,loss, predict_class, accuracy], feed_dict={x_image: batch_image_x, x_action: batch_action_x, y_class:batch_score_y,  keep: [0.0]})\n",
    "#              = sess.run(, feed_dict={x_image: batch_image_x, x_action: batch_action_x,y_label:batch_score_y,  keep: [0.0]} )\n",
    "#         print(\"batch:\",batch,\" accident_pred:\", predict_accident, \"accident_ground:\", batch_score_y)\n",
    "        epoch_loss.append(batch_loss)\n",
    "        epoch_acc.append(acc)\n",
    "    print (\"Loss:\", np.mean(epoch_loss), \" acc:\",np.mean(epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 2\n",
    "def train():\n",
    "    n_epochs = 100\n",
    "    train_num = 101\n",
    "    test_num = 20\n",
    "    batch_size = 6\n",
    "    x_image, x_action, y_class, keep, loss, optimizer, predict_class, accuracy = build_model()\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    save_path = \"model8/\"\n",
    "#     saver = tf.train.Saver(max_to_keep=100)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model8/model-5\")\n",
    "    for epoch in range(n_epochs):\n",
    "        # random chose batch.npz\n",
    "    #     epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        n_batchs = np.arange(1,train_num+1)\n",
    "        np.random.shuffle(n_batchs)\n",
    "        tStart_epoch = time.time()\n",
    "        for batch in n_batchs:\n",
    "            file_name = '%03d' %batch\n",
    "            batch_data = np.load(train_path+'batch_'+file_name+'.npz')\n",
    "            batch_image_x, batch_action_x, batch_score_y = build_data(batch_data)\n",
    "#             print(batch_image_x.shape, batch_action_x.shape, batch_score_y.shape)\n",
    "            \n",
    "            \n",
    "#             batch_image_x = batch_data['data'][:,137:237]\n",
    "#             batch_action_x = batch_data['action'][:,137:237]\n",
    "#             batch_score_y = batch_data['score'][:,150:]\n",
    "#             batch_score_y = revise_score(batch_score_y)\n",
    "#             print(batch_score_y.shape)\n",
    "            _,batch_loss, predict_accident, acc = sess.run([optimizer,loss, predict_class, accuracy], feed_dict={x_image: batch_image_x, x_action: batch_action_x, y_class:batch_score_y,  keep: [0.5]})\n",
    "#              = sess.run(, feed_dict={x_image: batch_image_x, x_action: batch_action_x,y_label:batch_score_y,  keep: [0.0]} )\n",
    "#             print(\"batch:\",batch,\" accident_pred:\", predict_accident, \"accident_ground:\", class_label)\n",
    "#             print(\"batch_loss\",batch_loss)\n",
    "            epoch_loss.append(batch_loss)\n",
    "            epoch_acc.append(acc)\n",
    "    \n",
    "         # print one epoch\n",
    "        print (\"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss), \" acc:\",np.mean(epoch_acc))\n",
    "        tStop_epoch = time.time()\n",
    "        print (\"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\")\n",
    "        sys.stdout.flush()\n",
    "        if (epoch+1) %5 == 0:\n",
    "            saver.save(sess,save_path+\"model\", global_step = epoch+1)\n",
    "            print (\"Training\")\n",
    "            test_all(sess, train_num, train_path, x_image, x_action, y_class,  keep, loss, optimizer, predict_class, accuracy)\n",
    "            print (\"Testing\")\n",
    "            test_all(sess, test_num, test_path, x_image, x_action, y_class, keep, loss, optimizer, predict_class, accuracy)\n",
    "    print (\"Optimization Finished!\")\n",
    "    saver.save(sess, save_path+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fb94619bfd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fb9461502e8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fb946150748>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Restoring parameters from model8/model-5\n",
      "Epoch: 1  done. Loss: 0.6929926  acc: 0.5445545\n",
      "Epoch Time Cost: 888.01 s\n",
      "Epoch: 2  done. Loss: 0.70588875  acc: 0.5198019\n",
      "Epoch Time Cost: 895.5 s\n",
      "Epoch: 3  done. Loss: 0.7027108  acc: 0.5165016\n",
      "Epoch Time Cost: 897.75 s\n",
      "Epoch: 4  done. Loss: 0.7081926  acc: 0.51980203\n",
      "Epoch Time Cost: 889.84 s\n",
      "Epoch: 5  done. Loss: 0.6954925  acc: 0.5346535\n",
      "Epoch Time Cost: 886.41 s\n",
      "Training\n",
      "Loss: 0.7095328  acc: 0.5049505\n",
      "Testing\n",
      "Loss: 0.7058879  acc: 0.45833334\n",
      "Epoch: 6  done. Loss: 0.7099879  acc: 0.49174914\n",
      "Epoch Time Cost: 907.28 s\n",
      "Epoch: 7  done. Loss: 0.7082317  acc: 0.490099\n",
      "Epoch Time Cost: 863.12 s\n",
      "Epoch: 8  done. Loss: 0.69864464  acc: 0.5231023\n",
      "Epoch Time Cost: 899.36 s\n",
      "Epoch: 9  done. Loss: 0.710563  acc: 0.5049505\n",
      "Epoch Time Cost: 892.28 s\n",
      "Epoch: 10  done. Loss: 0.7036791  acc: 0.5346535\n",
      "Epoch Time Cost: 911.6 s\n",
      "Training\n",
      "Loss: 0.70785016  acc: 0.5049505\n",
      "Testing\n",
      "Loss: 0.7064663  acc: 0.45833334\n",
      "Epoch: 11  done. Loss: 0.69748574  acc: 0.5231023\n",
      "Epoch Time Cost: 901.58 s\n",
      "Epoch: 12  done. Loss: 0.7080722  acc: 0.5445545\n",
      "Epoch Time Cost: 912.44 s\n",
      "Epoch: 13  done. Loss: 0.70957553  acc: 0.5066007\n",
      "Epoch Time Cost: 888.1 s\n",
      "Epoch: 14  done. Loss: 0.715634  acc: 0.50825083\n",
      "Epoch Time Cost: 919.96 s\n",
      "Epoch: 15  done. Loss: 0.7137594  acc: 0.5165016\n",
      "Epoch Time Cost: 883.46 s\n",
      "Training\n",
      "Loss: 0.7070402  acc: 0.50165015\n",
      "Testing\n",
      "Loss: 0.70717025  acc: 0.45833334\n",
      "Epoch: 16  done. Loss: 0.71310246  acc: 0.519802\n",
      "Epoch Time Cost: 902.45 s\n",
      "Epoch: 17  done. Loss: 0.70662045  acc: 0.5132013\n",
      "Epoch Time Cost: 902.26 s\n",
      "Epoch: 18  done. Loss: 0.7174249  acc: 0.5049505\n",
      "Epoch Time Cost: 917.3 s\n",
      "Epoch: 19  done. Loss: 0.7057469  acc: 0.5313531\n",
      "Epoch Time Cost: 901.88 s\n",
      "Epoch: 20  done. Loss: 0.7025424  acc: 0.53960395\n",
      "Epoch Time Cost: 881.52 s\n",
      "Training\n",
      "Loss: 0.7079007  acc: 0.50825083\n",
      "Testing\n",
      "Loss: 0.7076208  acc: 0.47500005\n",
      "Epoch: 21  done. Loss: 0.694217  acc: 0.5346535\n",
      "Epoch Time Cost: 899.58 s\n",
      "Epoch: 22  done. Loss: 0.7104384  acc: 0.50660074\n",
      "Epoch Time Cost: 901.03 s\n",
      "Epoch: 23  done. Loss: 0.6970944  acc: 0.5214521\n",
      "Epoch Time Cost: 889.51 s\n",
      "Epoch: 24  done. Loss: 0.7337523  acc: 0.49834982\n",
      "Epoch Time Cost: 886.96 s\n",
      "Epoch: 25  done. Loss: 0.7036111  acc: 0.52640265\n",
      "Epoch Time Cost: 878.58 s\n",
      "Training\n",
      "Loss: 0.7061745  acc: 0.50825083\n",
      "Testing\n",
      "Loss: 0.70749724  acc: 0.45833334\n",
      "Epoch: 26  done. Loss: 0.6979906  acc: 0.5363036\n",
      "Epoch Time Cost: 896.09 s\n",
      "Epoch: 27  done. Loss: 0.7124088  acc: 0.52145207\n",
      "Epoch Time Cost: 880.1 s\n",
      "Epoch: 28  done. Loss: 0.712532  acc: 0.51650167\n",
      "Epoch Time Cost: 880.72 s\n",
      "Epoch: 29  done. Loss: 0.713051  acc: 0.5181518\n",
      "Epoch Time Cost: 869.73 s\n",
      "Epoch: 30  done. Loss: 0.7126703  acc: 0.48514855\n",
      "Epoch Time Cost: 897.01 s\n",
      "Training\n",
      "Loss: 0.71126777  acc: 0.49504954\n",
      "Testing\n",
      "Loss: 0.70671564  acc: 0.45833334\n",
      "Epoch: 31  done. Loss: 0.699988  acc: 0.5561056\n",
      "Epoch Time Cost: 899.4 s\n",
      "Epoch: 32  done. Loss: 0.714373  acc: 0.5099009\n",
      "Epoch Time Cost: 885.6 s\n",
      "Epoch: 33  done. Loss: 0.7115889  acc: 0.51155114\n",
      "Epoch Time Cost: 884.06 s\n",
      "Epoch: 34  done. Loss: 0.7169978  acc: 0.49504948\n",
      "Epoch Time Cost: 884.21 s\n",
      "Epoch: 35  done. Loss: 0.7060813  acc: 0.5181518\n",
      "Epoch Time Cost: 894.69 s\n",
      "Training\n",
      "Loss: 0.7099151  acc: 0.49834982\n",
      "Testing\n",
      "Loss: 0.70768666  acc: 0.45833334\n",
      "Epoch: 36  done. Loss: 0.70693284  acc: 0.50000006\n",
      "Epoch Time Cost: 880.49 s\n",
      "Epoch: 37  done. Loss: 0.7152021  acc: 0.50825083\n",
      "Epoch Time Cost: 890.44 s\n",
      "Epoch: 38  done. Loss: 0.7140192  acc: 0.5313532\n",
      "Epoch Time Cost: 903.89 s\n",
      "Epoch: 39  done. Loss: 0.71406436  acc: 0.48679864\n",
      "Epoch Time Cost: 897.48 s\n",
      "Epoch: 40  done. Loss: 0.70212066  acc: 0.51980203\n",
      "Epoch Time Cost: 900.14 s\n",
      "Training\n",
      "Loss: 0.71042955  acc: 0.50825083\n",
      "Testing\n",
      "Loss: 0.70811343  acc: 0.47500005\n",
      "Epoch: 41  done. Loss: 0.71125954  acc: 0.4966997\n",
      "Epoch Time Cost: 905.24 s\n",
      "Epoch: 42  done. Loss: 0.7198263  acc: 0.4834983\n",
      "Epoch Time Cost: 894.64 s\n",
      "Epoch: 43  done. Loss: 0.710434  acc: 0.5280528\n",
      "Epoch Time Cost: 892.91 s\n",
      "Epoch: 44  done. Loss: 0.7061391  acc: 0.51485145\n",
      "Epoch Time Cost: 876.66 s\n",
      "Epoch: 45  done. Loss: 0.7104127  acc: 0.52475244\n",
      "Epoch Time Cost: 891.56 s\n",
      "Training\n",
      "Loss: 0.7076902  acc: 0.50165015\n",
      "Testing\n",
      "Loss: 0.7073371  acc: 0.45833334\n",
      "Epoch: 46  done. Loss: 0.71581364  acc: 0.5049505\n",
      "Epoch Time Cost: 882.62 s\n",
      "Epoch: 47  done. Loss: 0.7039687  acc: 0.5346535\n",
      "Epoch Time Cost: 915.56 s\n",
      "Epoch: 48  done. Loss: 0.714287  acc: 0.509901\n",
      "Epoch Time Cost: 899.69 s\n",
      "Epoch: 49  done. Loss: 0.7021194  acc: 0.53300333\n",
      "Epoch Time Cost: 874.48 s\n",
      "Epoch: 50  done. Loss: 0.724329  acc: 0.47194716\n",
      "Epoch Time Cost: 883.99 s\n",
      "Training\n",
      "Loss: 0.71227133  acc: 0.49834985\n",
      "Testing\n",
      "Loss: 0.7069561  acc: 0.45833334\n",
      "Epoch: 51  done. Loss: 0.70676273  acc: 0.5148515\n",
      "Epoch Time Cost: 891.2 s\n",
      "Epoch: 52  done. Loss: 0.7137652  acc: 0.5247525\n",
      "Epoch Time Cost: 906.06 s\n",
      "Epoch: 53  done. Loss: 0.7198554  acc: 0.5049505\n",
      "Epoch Time Cost: 889.97 s\n",
      "Epoch: 54  done. Loss: 0.7044821  acc: 0.5181518\n",
      "Epoch Time Cost: 890.36 s\n",
      "Epoch: 55  done. Loss: 0.70499104  acc: 0.5330033\n",
      "Epoch Time Cost: 872.2 s\n",
      "Training\n",
      "Loss: 0.70892847  acc: 0.51155114\n",
      "Testing\n",
      "Loss: 0.70815164  acc: 0.45833334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-9c329d9ef398>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%03d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'batch_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mbatch_image_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_action_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_score_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#             print(batch_image_x.shape, batch_action_x.shape, batch_score_y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-775f33bb2480>\u001b[0m in \u001b[0;36mbuild_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbatch_action_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mbatch_image_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mbatch_score_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_image_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_action_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_score_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    234\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    685\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
