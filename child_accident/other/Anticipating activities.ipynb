{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Global Parameters ###############\n",
    "# path\n",
    "train_path = '/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/training/'\n",
    "test_path = '/media/user/Hard_Disk/Dataset/child_accident_2/batch_start2end_score/testing/'\n",
    "# demo_path = './dataset/features/testing/'\n",
    "# default_model_path = './model/demo_model'\n",
    "save_path = './model2/'\n",
    "# video_path = './dataset/videos/training/positive/'\n",
    "# batch_number\n",
    "train_num = 101\n",
    "test_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRNN:\n",
    "    def __init__(self, n_classes, rnn_size, max_seq_sz, num_layers):\n",
    "        self.input_seq = tf.placeholder('float', [None, max_seq_sz, n_classes])\n",
    "        self.target = tf.placeholder('float', [None, n_classes])\n",
    "        self.nClasses = n_classes\n",
    "        self.rnn_size = rnn_size\n",
    "        self.max_seq_sz = max_seq_sz\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.__build()\n",
    "\n",
    "    def __weight_variable(self, shape, myName):\n",
    "        initial = tf.random_normal(shape, stddev=0.1, name=myName)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def __bias_variable(self, shape, myName):\n",
    "        initial = tf.constant(0.1, shape=shape, name=myName)\n",
    "        return tf.Variable(initial)\n",
    "        \n",
    "    def __build(self):\n",
    "        w_fc_in = self.__weight_variable([self.nClasses, 128], 'w_fc_in')\n",
    "        b_fc_in = self.__bias_variable([128], 'b_fc_in')\n",
    "        \n",
    "        w_fc_o = self.__weight_variable([self.rnn_size, 128], 'w_fc_o')\n",
    "        b_fc_o = self.__bias_variable([128], 'b_fc_o')\n",
    "                \n",
    "        w_output_action = self.__weight_variable([128, self.nClasses], 'w_fc_in')\n",
    "        b_output_action = self.__bias_variable([self.nClasses], 'b_fc_in')\n",
    "        \n",
    "        w_output_len = self.__weight_variable([128, 2], 'w_fc_in')\n",
    "        b_output_len = self.__bias_variable([2], 'b_fc_in')\n",
    "        \n",
    "        x = tf.reshape(self.input_seq, [-1, self.nClasses])\n",
    "        h1 = tf.nn.relu(tf.matmul(x, w_fc_in) + b_fc_in)\n",
    "        h1 = tf.reshape(h1, [-1,self.max_seq_sz,128])\n",
    "        #rnn\n",
    "        h1 = tf.unstack(h1, axis=1)\n",
    "        def get_cell():\n",
    "            return rnn.GRUCell(self.rnn_size)   \n",
    "        gru_cell = rnn.MultiRNNCell([get_cell() for _ in range(self.num_layers)])\n",
    "        outputs, states = rnn.static_rnn(gru_cell, h1, dtype=tf.float32) \n",
    "        #fc_o\n",
    "        h2 = tf.nn.relu(tf.matmul(outputs[-1], w_fc_o) + b_fc_o)\n",
    "        #output\n",
    "        output_label = tf.matmul(h2, w_output_action) + b_output_action\n",
    "        output_len = tf.nn.relu(tf.matmul(h2, w_output_len) + b_output_len)\n",
    "        #    \n",
    "        self.prediction = tf.concat([output_label, output_len], 1)\n",
    "        self.saver = tf.train.Saver(write_version=tf.train.SaverDef.V2, max_to_keep=100)\n",
    "    \n",
    "    def train(self, sess, model_save_path, batch_gen, nEpochs, save_freq, batch_size):\n",
    "        gt_labels = self.target[:,:-2]\n",
    "        gt_length = self.target[:,-2:]\n",
    "        predicted_labels = self.prediction[:,:-2]\n",
    "        predicted_length = self.prediction[:,-2:]\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=gt_labels, logits=predicted_labels, dim=1))\n",
    "        loss += tf.reduce_mean(tf.square( gt_length - predicted_length ))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "                \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "      \n",
    "        for epoch in range(nEpochs):\n",
    "            epoch_loss = 0\n",
    "            i=0\n",
    "            while(batch_gen.has_next()):\n",
    "                batch_in, batch_target = batch_gen.next_batch(batch_size)\n",
    "                _, err = sess.run([optimizer, loss], feed_dict={self.input_seq: batch_in, self.target: batch_target})\n",
    "                i=i+1\n",
    "                epoch_loss += err       \n",
    "            batch_gen.reset()\n",
    "            \n",
    "            if epoch%save_freq==0:  \n",
    "                print ('Epoch', (epoch+1), 'completed out of',nEpochs,'epoch loss: %.2f'%(epoch_loss/i))\n",
    "                path = model_save_path+\"/epoch-\"+str(epoch+1)\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                self.saver.save(sess, path+\"/model.ckpt\")    \n",
    "    \n",
    "    \n",
    "\n",
    "    def predict(self, sess, model_save_path, pred_len, label_seq, length_seq, actions_dict, T):\n",
    "        self.saver.restore(sess, model_save_path)\n",
    "        l = 0\n",
    "        while l < pred_len:\n",
    "            p_seq = np.zeros((self.max_seq_sz, self.nClasses+1))\n",
    "            for i in range(len(label_seq[-self.max_seq_sz:])):\n",
    "                p_seq[i][-1] = length_seq[i]/T\n",
    "                p_seq[i][ actions_dict[label_seq[i]] ] = 1\n",
    "\n",
    "            result = self.prediction.eval({self.input_seq:[p_seq]})[0]\n",
    "\n",
    "            if int(result[-1]*T) > 0:\n",
    "                length_seq[-1] += result[-1]*T\n",
    "                l = l + int(result[-1]*T)\n",
    "            if int(result[-2]*T) > 0:\n",
    "                l = l + int(result[-2]*T)\n",
    "                label_seq.append(actions_dict.keys()[actions_dict.values().index(np.argmax(result[:-2]))])\n",
    "                length_seq.append(result[-2]*T)\n",
    "            if int(result[-1]*T) == 0 and int(result[-2]*T) == 0:\n",
    "                l = l+pred_len\n",
    "                length_seq[-1] += pred_len\n",
    "\n",
    "        return label_seq, length_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_batch_generator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.list_of_examples = []\n",
    "        self.index = 0\n",
    "    \n",
    "    def number_of_examples(self):\n",
    "        return len(self.list_of_examples)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return\n",
    "        \n",
    "    def has_next(self):\n",
    "        if self.index < len(self.list_of_examples):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def read_data(self, list_of_videos):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_batch_generator(Base_batch_generator):\n",
    "    \n",
    "    def __init__(self, nClasses, n_iterations, max_seq_sz, actions_dict, alpha):\n",
    "        super(RNN_batch_generator, self).__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.nClasses = nClasses\n",
    "        self.max_seq_sz = max_seq_sz\n",
    "        self.actions_dict = actions_dict\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def read_data(self, list_of_videos):\n",
    "        \n",
    "        for vid in list_of_videos:\n",
    "                \n",
    "            file_ptr = open(vid, 'r')\n",
    "            content = file_ptr.read().split('\\n')[:-1]\n",
    "            \n",
    "            label_seq, length_seq = get_label_length_seq(content) \n",
    "            T = (1.0/self.alpha)*len(content)\n",
    "        \n",
    "            for itr in range(self.n_iterations):\n",
    "                #list of partial length of each label in the sequence\n",
    "                rand_cuts = []\n",
    "                for i in range(len(label_seq)-1):\n",
    "                    rand_cuts.append( int( length_seq[i] * float(itr+.5)/self.n_iterations  ) )\n",
    "                    \n",
    "                for i in range(len(rand_cuts)):\n",
    "                    seq_len = i+1\n",
    "                    p_seq = []\n",
    "                    for j in range(seq_len):\n",
    "                        p_seq.append(np.zeros((self.nClasses+1)))\n",
    "                        if j == seq_len-1:\n",
    "                            p_seq[-1][-1] = rand_cuts[j]/T\n",
    "                        else:\n",
    "                            p_seq[-1][-1] = length_seq[j]/T\n",
    "                        p_seq[-1][self.actions_dict[label_seq[j]]] = 1\n",
    "                        \n",
    "                    for j in range(self.max_seq_sz - seq_len):\n",
    "                        p_seq.append(np.zeros((self.nClasses+1)))\n",
    "                    \n",
    "                    p_tar = np.zeros((self.nClasses+2))\n",
    "                    #target length\n",
    "                    if i != len(rand_cuts)-1:\n",
    "                        p_tar[-2] = rand_cuts[i+1]/T\n",
    "                    else:\n",
    "                        p_tar[-2] = length_seq[i+1]/T\n",
    "                    #remaining length\n",
    "                    p_tar[-1] = (length_seq[i]-rand_cuts[i])/T\n",
    "                    #target action\n",
    "                    p_tar[ self.actions_dict[label_seq[i+1]] ] = 1\n",
    "                    \n",
    "                    example = [p_seq, p_tar, seq_len]\n",
    "                    self.list_of_examples.append(example)\n",
    "                    \n",
    "        random.shuffle(self.list_of_examples) \n",
    "        return\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch = np.array( sorted(self.list_of_examples[self.index:self.index+batch_size], key=lambda x: x[2], reverse=True) )\n",
    "        self.index += batch_size\n",
    "        batch_vid = list(batch[:,0])\n",
    "        batch_target = list(batch[:,1])\n",
    "                \n",
    "        return batch_vid, batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRNN(2, 512, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
