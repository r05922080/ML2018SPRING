{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    def __init__(self, seq_length, batch_size):\n",
    "        np.random.seed(100)\n",
    "        self.seq_length = seq_length\n",
    "        self.acc_num = 334\n",
    "        self.no_acc_num = 392\n",
    "        self.split = 0.95\n",
    "        self.train_index = 0 \n",
    "        self.train_batch_size = batch_size\n",
    "        self.valid_index = 0\n",
    "        self.valid_batch_size = 10\n",
    "        self.read_annotation()\n",
    "        self.shuffle_data()\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        #img_range = np.arange(0,300-self.seq_length-60,60)\n",
    "        acc_list = np.arange(1,self.acc_num+1)\n",
    "        no_acc_list = np.arange(1,self.no_acc_num+1)\n",
    "        list1 = np.array(np.meshgrid(1,acc_list)).T.reshape(-1,2)\n",
    "        list2 = np.array(np.meshgrid(0,no_acc_list)).T.reshape(-1,2)\n",
    "        shuffle_list = np.concatenate([list1, list2], axis=0)\n",
    "        np.random.shuffle(shuffle_list)\n",
    "        self.train = shuffle_list[:int(shuffle_list.shape[0]*self.split)]\n",
    "        self.valid = shuffle_list[int(shuffle_list.shape[0]*self.split):] \n",
    "        \n",
    "    def read_annotation(self):\n",
    "        annotation_file = '/media/user/Hard_Disk/Dataset/child_accident_2/annotation/accident_frame.txt'\n",
    "        w = open(annotation_file, \"r\")\n",
    "        ann = w.read()\n",
    "        annotation_data = []\n",
    "        for i in ann.split(\"\\n\"):\n",
    "            b = i.split(\" \")\n",
    "            if (len(b) > 1):\n",
    "                annotation_data.append(b[1])\n",
    "        self.annotation = np.array(annotation_data).astype(\"int32\")\n",
    "        \n",
    "    def read_data(self, is_accident, dir_index):\n",
    "        data = []\n",
    "        label = []\n",
    "        img_path = '/media/user/Hard_Disk/Dataset/child_accident_2/image feature/' \n",
    "        act_path = '/media/user/Hard_Disk/Dataset/child_accident_2/action feature/' \n",
    "        if (is_accident):\n",
    "            acc_dir = \"accident/\"\n",
    "            range_start = self.annotation[dir_index-1] - self.seq_length\n",
    "            range_end = self.annotation[dir_index-1]\n",
    "            label.append([0,1])\n",
    "            \n",
    "        else:\n",
    "            acc_dir = \"no_accident/\"\n",
    "            range_start = 0\n",
    "            range_end = self.seq_length\n",
    "            label.append([1,0])\n",
    "            \n",
    "        dir_name = \"%04d\"%dir_index\n",
    "        img_npy = []\n",
    "        act_npy = []\n",
    "        for j in range(range_start, range_end):\n",
    "            img_feature = np.load(img_path + acc_dir + dir_name + \"/\" + str(j) + \".npy\")\n",
    "            img_npy.append(img_feature)\n",
    "            act_feature = np.load(act_path + acc_dir + dir_name + \"/\" + str(j) + \".npy\")\n",
    "            act_npy.append(act_feature)\n",
    "        \n",
    "            \n",
    "        return np.array(img_npy), np.array(act_npy) ,np.array(label)\n",
    "        \n",
    "    def next_batch(self, mode=\"train\"):\n",
    "        batch_img = []\n",
    "        batch_act = []\n",
    "        batch_y = []\n",
    "        if (mode == \"valid\"):\n",
    "            batch_size = self.valid_batch_size\n",
    "        elif (mode == \"train\"):\n",
    "            batch_size = self.train_batch_size\n",
    "        for i in range(batch_size):\n",
    "            if (mode == \"train\"):\n",
    "                img, act, label = self.read_data(self.train[self.train_index+i][0], self.train[self.train_index+i][1])\n",
    "            elif (mode == \"valid\"):\n",
    "                img, act, label = self.read_data(self.valid[self.valid_index+i][0], self.valid[self.valid_index+i][1])\n",
    "            batch_img.append(img)\n",
    "            batch_act.append(act)\n",
    "            batch_y.append(label)\n",
    "        if (mode == \"valid\"):\n",
    "            self.valid_index += self.valid_batch_size\n",
    "        elif (mode == \"train\"):\n",
    "            self.train_index += self.train_batch_size\n",
    "        return np.array(batch_img), np.array(batch_act), np.squeeze(np.array(batch_y))\n",
    "    \n",
    "    def has_next(self, mode=\"train\"):\n",
    "        if (mode == \"train\"):\n",
    "            if (self.train_index + self.train_batch_size >= self.train.shape[0]):\n",
    "                return False\n",
    "        elif (mode == \"valid\"):\n",
    "            if (self.valid_index + self.valid_batch_size >= self.valid.shape[0]):\n",
    "                return False\n",
    "        return True\n",
    "    def display_shape(self):\n",
    "        print(\"train shape:\",self.train.shape, \" valid shape:\",self.valid.shape)\n",
    "        \n",
    "    def reset_batch(self, mode=\"train\"):\n",
    "        if (mode == \"train\"):\n",
    "            self.train_index = 0\n",
    "            np.random.shuffle(self.train)\n",
    "        elif (mode == \"valid\"):\n",
    "            self.valid_index = 0\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, img_input_size, act_input_size, hidden_size, embedding_size, embedding_size2, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.img = nn.Sequential(\n",
    "            nn.Linear(img_input_size, embedding_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size2)\n",
    "        )\n",
    "        self.act = nn.Sequential(\n",
    "            nn.Linear(act_input_size, embedding_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size2)\n",
    "        )\n",
    "            \n",
    "        \n",
    "        self.img_fc1 = nn.Linear(img_input_size, embedding_size)\n",
    "        self.act_fc1 = nn.Linear(act_input_size, embedding_size)\n",
    "        self.img_fc2 = nn.Linear(embedding_size, embedding_size2)\n",
    "        self.act_fc2 = nn.Linear(embedding_size, embedding_size2)\n",
    "        self.lstm1 = nn.LSTM(embedding_size2*2, hidden_size, num_layers,dropout=0.25, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(embedding_size2*2, hidden_size, num_layers,dropout=0.25, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(embedding_size2*2, hidden_size, num_layers,dropout=0.25, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(embedding_size2*2, hidden_size, num_layers,dropout=0.25, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(embedding_size2*2, hidden_size, num_layers,dropout=0.25, batch_first=True)\n",
    "        self.st5_state_h = nn.Linear(hidden_size*5, hidden_size)\n",
    "        self.st5_state_c = nn.Linear(hidden_size*5, hidden_size)\n",
    "        self.st4_state_h = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.st4_state_c = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.st3_state_h = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.st3_state_c = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.st2_state_h = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.st2_state_c = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # Set initial hidden and cell states \n",
    "        x1 = self.img_fc1(x)\n",
    "        x1 = self.img_fc2(x1)\n",
    "        x2 = self.act_fc1(y)\n",
    "        x2 = self.act_fc2(x2)\n",
    "#         x1 = self.img(x)\n",
    "#         x = self.act(y)\n",
    "        x = torch.cat((x1, x2), 2)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        st1_h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        st1_c = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        st2_h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        st2_c = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        st3_h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        st3_c = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        st4_h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        st4_c = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        st5_h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        st5_c = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        for i in range(x.size(1)):\n",
    "        # Forward propagate LSTM\n",
    "            stack1_out, (st1_h, st1_c) = self.lstm1(x[:,i].unsqueeze(1), (st1_h, st1_c))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "            stack2_out, (st2_h, st2_c) = self.lstm2(x[:,int(i/10)].unsqueeze(1), (st2_h, st2_c))\n",
    "            stack3_out, (st3_h, st3_c) = self.lstm3(x[:,int(i/20)].unsqueeze(1), (st3_h, st3_c))\n",
    "            stack4_out, (st4_h, st4_c) = self.lstm3(x[:,int(i/30)].unsqueeze(1), (st4_h, st4_c))\n",
    "            stack5_out, (st5_h, st5_c) = self.lstm3(x[:,int(i/40)].unsqueeze(1), (st5_h, st5_c))\n",
    "            \n",
    "            st5_h = torch.cat((st1_h, st2_h, st3_h, st4_h, st5_h), 2)\n",
    "            st5_h = self.st5_state_h(st5_h)\n",
    "            \n",
    "            st5_c = torch.cat((st1_c, st2_c, st3_c, st4_c, st5_c), 2)\n",
    "            st5_c = self.st5_state_c(st5_c)\n",
    "            \n",
    "            st4_h = torch.cat((st4_h, st4_h, st4_h, st4_h), 2)\n",
    "            st4_h = self.st4_state_h(st4_h)\n",
    "            \n",
    "            st4_c = torch.cat((st1_c, st2_c, st3_c, st4_c), 2)\n",
    "            st4_c = self.st4_state_c(st4_c)\n",
    "            \n",
    "            st3_h = torch.cat((st1_h, st2_h, st3_h), 2)\n",
    "            st3_h = self.st3_state_h(st3_h)\n",
    "            \n",
    "            st3_c = torch.cat((st1_c, st2_c, st3_c), 2)\n",
    "            st3_c = self.st3_state_c(st3_c)\n",
    "            \n",
    "            \n",
    "            st2_h = torch.cat((st1_h, st2_h), 2)\n",
    "            st2_h = self.st2_state_h(st2_h)\n",
    "            \n",
    "            st2_c = torch.cat((st1_c, st2_c), 2)\n",
    "            st2_c = self.st2_state_c(st2_c)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        \n",
    "        out = self.fc2(stack5_out[:,-1,:])\n",
    "        \n",
    "#         out = self.fc2(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNN(img_input_size=20*4096, act_input_size=1024, hidden_size=1024, embedding_size=1024, embedding_size2=512, num_layers=1, num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.from_numpy(np.array([1,1])).type(torch.FloatTensor).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (689, 2)  valid shape: (37, 2)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 100\n",
    "num_epochs = 20\n",
    "train_data = data(seq_length=sequence_length, batch_size=50)\n",
    "train_data.display_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    i = 0\n",
    "    acc = 0\n",
    "    while(train_data.has_next(\"valid\")):\n",
    "        img, act, labels = train_data.next_batch(\"valid\")\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        img = torch.from_numpy(img.reshape(-1, sequence_length, 20*4096)).to(device)\n",
    "        act = torch.from_numpy(np.squeeze(act)).to(device)\n",
    "        outputs = model(img, act)\n",
    "        outputs = F.softmax(outputs)\n",
    "        predict = torch.max(outputs, 1)[1]\n",
    "        target = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, target)\n",
    "        correct = (predict == target).squeeze()\n",
    "        acc += torch.nonzero(correct).size(0) / predict.shape[0]\n",
    "        i += 1\n",
    "    train_data.reset_batch(\"valid\")\n",
    "    print(\"validation accuracy:\",acc/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step 5, Loss: 0.6786\n",
      "train accuracy: 0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.5666666666666667\n",
      "====================================\n",
      "Epoch [1/20], Step 10, Loss: 0.6872\n",
      "train accuracy: 0.4844444444444444\n",
      "validation accuracy: 0.6333333333333333\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Epoch [2/20], Step 5, Loss: 0.6753\n",
      "train accuracy: 0.6799999999999999\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "Epoch [2/20], Step 10, Loss: 0.6861\n",
      "train accuracy: 0.6599999999999999\n",
      "validation accuracy: 0.6999999999999998\n",
      "====================================\n",
      "epoch: 2\n",
      "Epoch [3/20], Step 5, Loss: 0.6082\n",
      "train accuracy: 0.685\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "Epoch [3/20], Step 10, Loss: 0.5612\n",
      "train accuracy: 0.6866666666666668\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "epoch: 3\n",
      "Epoch [4/20], Step 5, Loss: 0.5703\n",
      "train accuracy: 0.74\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [4/20], Step 10, Loss: 0.5419\n",
      "train accuracy: 0.7511111111111111\n",
      "validation accuracy: 0.8333333333333334\n",
      "====================================\n",
      "epoch: 4\n",
      "Epoch [5/20], Step 5, Loss: 0.6001\n",
      "train accuracy: 0.7250000000000001\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [5/20], Step 10, Loss: 0.5053\n",
      "train accuracy: 0.7688888888888888\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 5\n",
      "Epoch [6/20], Step 5, Loss: 0.4858\n",
      "train accuracy: 0.7949999999999999\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [6/20], Step 10, Loss: 0.4645\n",
      "train accuracy: 0.8\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 6\n",
      "Epoch [7/20], Step 5, Loss: 0.4532\n",
      "train accuracy: 0.835\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [7/20], Step 10, Loss: 0.5039\n",
      "train accuracy: 0.8488888888888888\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 7\n",
      "Epoch [8/20], Step 5, Loss: 0.5116\n",
      "train accuracy: 0.835\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [8/20], Step 10, Loss: 0.4053\n",
      "train accuracy: 0.8488888888888889\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "epoch: 8\n",
      "Epoch [9/20], Step 5, Loss: 0.5051\n",
      "train accuracy: 0.8999999999999999\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "Epoch [9/20], Step 10, Loss: 0.4243\n",
      "train accuracy: 0.8911111111111111\n",
      "validation accuracy: 0.7666666666666667\n",
      "====================================\n",
      "epoch: 9\n",
      "Epoch [10/20], Step 5, Loss: 0.4554\n",
      "train accuracy: 0.8899999999999999\n",
      "validation accuracy: 0.7666666666666667\n",
      "====================================\n",
      "Epoch [10/20], Step 10, Loss: 0.3906\n",
      "train accuracy: 0.9066666666666667\n",
      "validation accuracy: 0.7666666666666667\n",
      "====================================\n",
      "epoch: 10\n",
      "Epoch [11/20], Step 5, Loss: 0.3824\n",
      "train accuracy: 0.95\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [11/20], Step 10, Loss: 0.3788\n",
      "train accuracy: 0.94\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "epoch: 11\n",
      "Epoch [12/20], Step 5, Loss: 0.4164\n",
      "train accuracy: 0.945\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [12/20], Step 10, Loss: 0.3230\n",
      "train accuracy: 0.9666666666666666\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "epoch: 12\n",
      "Epoch [13/20], Step 5, Loss: 0.3409\n",
      "train accuracy: 0.945\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [13/20], Step 10, Loss: 0.3387\n",
      "train accuracy: 0.9688888888888889\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "epoch: 13\n",
      "Epoch [14/20], Step 5, Loss: 0.3175\n",
      "train accuracy: 0.935\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "Epoch [14/20], Step 10, Loss: 0.4190\n",
      "train accuracy: 0.9244444444444445\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 14\n",
      "Epoch [15/20], Step 5, Loss: 0.3228\n",
      "train accuracy: 0.945\n",
      "validation accuracy: 0.7999999999999999\n",
      "====================================\n",
      "Epoch [15/20], Step 10, Loss: 0.3855\n",
      "train accuracy: 0.9444444444444444\n",
      "validation accuracy: 0.7333333333333334\n",
      "====================================\n",
      "epoch: 15\n",
      "Epoch [16/20], Step 5, Loss: 0.5153\n",
      "train accuracy: 0.8799999999999999\n",
      "validation accuracy: 0.7000000000000001\n",
      "====================================\n",
      "Epoch [16/20], Step 10, Loss: 0.3476\n",
      "train accuracy: 0.9200000000000002\n",
      "validation accuracy: 0.7999999999999999\n",
      "====================================\n",
      "epoch: 16\n",
      "Epoch [17/20], Step 5, Loss: 0.3546\n",
      "train accuracy: 0.965\n",
      "validation accuracy: 0.7666666666666667\n",
      "====================================\n",
      "Epoch [17/20], Step 10, Loss: 0.3739\n",
      "train accuracy: 0.9555555555555555\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "epoch: 17\n",
      "Epoch [18/20], Step 5, Loss: 0.3581\n",
      "train accuracy: 0.97\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [18/20], Step 10, Loss: 0.3145\n",
      "train accuracy: 0.9666666666666666\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "epoch: 18\n",
      "Epoch [19/20], Step 5, Loss: 0.3541\n",
      "train accuracy: 0.96\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [19/20], Step 10, Loss: 0.3704\n",
      "train accuracy: 0.9688888888888889\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 19\n",
      "Epoch [20/20], Step 5, Loss: 0.3154\n",
      "train accuracy: 0.965\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [20/20], Step 10, Loss: 0.3468\n",
      "train accuracy: 0.9733333333333333\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "epoch: 20\n",
      "Epoch [21/20], Step 5, Loss: 0.3286\n",
      "train accuracy: 0.96\n",
      "validation accuracy: 0.7666666666666666\n",
      "====================================\n",
      "Epoch [21/20], Step 10, Loss: 0.3541\n",
      "train accuracy: 0.9666666666666666\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 21\n",
      "Epoch [22/20], Step 5, Loss: 0.3139\n",
      "train accuracy: 0.985\n",
      "validation accuracy: 0.8333333333333334\n",
      "====================================\n",
      "Epoch [22/20], Step 10, Loss: 0.3142\n",
      "train accuracy: 0.9755555555555555\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 22\n",
      "Epoch [23/20], Step 5, Loss: 0.3538\n",
      "train accuracy: 0.96\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [23/20], Step 10, Loss: 0.3736\n",
      "train accuracy: 0.9688888888888889\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 23\n",
      "Epoch [24/20], Step 5, Loss: 0.3336\n",
      "train accuracy: 0.97\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [24/20], Step 10, Loss: 0.3336\n",
      "train accuracy: 0.9777777777777779\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 24\n",
      "Epoch [25/20], Step 5, Loss: 0.3136\n",
      "train accuracy: 0.985\n",
      "validation accuracy: 0.7666666666666667\n",
      "====================================\n",
      "Epoch [25/20], Step 10, Loss: 0.3136\n",
      "train accuracy: 0.9844444444444446\n",
      "validation accuracy: 0.7666666666666667\n",
      "====================================\n",
      "epoch: 25\n",
      "Epoch [26/20], Step 5, Loss: 0.3335\n",
      "train accuracy: 0.975\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [26/20], Step 10, Loss: 0.3535\n",
      "train accuracy: 0.9755555555555555\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 26\n",
      "Epoch [27/20], Step 5, Loss: 0.3335\n",
      "train accuracy: 0.97\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [27/20], Step 10, Loss: 0.3135\n",
      "train accuracy: 0.9733333333333333\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 27\n",
      "Epoch [28/20], Step 5, Loss: 0.3135\n",
      "train accuracy: 0.98\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [28/20], Step 10, Loss: 0.3734\n",
      "train accuracy: 0.9711111111111111\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 28\n",
      "Epoch [29/20], Step 5, Loss: 0.3134\n",
      "train accuracy: 0.97\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [29/20], Step 10, Loss: 0.3134\n",
      "train accuracy: 0.9755555555555555\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29\n",
      "Epoch [30/20], Step 5, Loss: 0.3334\n",
      "train accuracy: 0.98\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [30/20], Step 10, Loss: 0.3134\n",
      "train accuracy: 0.9777777777777779\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "epoch: 30\n",
      "Epoch [31/20], Step 5, Loss: 0.3334\n",
      "train accuracy: 0.99\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n",
      "Epoch [31/20], Step 10, Loss: 0.3334\n",
      "train accuracy: 0.9800000000000002\n",
      "validation accuracy: 0.8000000000000002\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "# Train the model\n",
    "\n",
    "for epoch in range(101):\n",
    "    i = 0\n",
    "    acc = 0\n",
    "    print(\"epoch:\",epoch)\n",
    "    while (train_data.has_next(\"train\")):\n",
    "        img, act, labels = train_data.next_batch(\"train\")\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        \n",
    "        img = torch.from_numpy(img.reshape(-1, sequence_length, 20*4096)).to(device)\n",
    "        act = torch.from_numpy(np.squeeze(act)).to(device)\n",
    "        outputs = model(img, act)\n",
    "        outputs = F.softmax(outputs)\n",
    "#         print(outputs.shape)\n",
    "#         outputs, torch.max(labels, 1)[1]\n",
    "        \n",
    "        predict = torch.max(outputs, 1)[1]\n",
    "        target = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        \n",
    "        correct = (predict == target).squeeze()\n",
    "#         print(predict)\n",
    "#         print(target)\n",
    "#         print(\"================\")\n",
    "#         print(correct)\n",
    "        acc += torch.nonzero(correct).size(0) / predict.shape[0]\n",
    "        \n",
    "    \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        if (i+1) % 5 == 0:\n",
    "            \n",
    "            print ('Epoch [{}/{}], Step {}, Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, loss.item()))\n",
    "            print ('train accuracy:',acc/i)\n",
    "            evaluation(model)\n",
    "            print(\"====================================\")\n",
    "    if (epoch % 10 == 0):\n",
    "        path = \"pytorch - 07/model-\"+str(epoch)+\".ckpt\"\n",
    "        torch.save(model, path)\n",
    "    train_data.reset_batch(\"train\")\n",
    "# # Test the model\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
